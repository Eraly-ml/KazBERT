#!/usr/bin/env python
import os
import torch
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from transformers import (
    AutoTokenizer,
    AutoModelForMaskedLM,
    Trainer,
    TrainingArguments
)
from datasets import load_dataset

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CPU
os.environ["OMP_NUM_THREADS"] = "8"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ CUDA-—Ñ—É–Ω–∫—Ü–∏–π
os.environ["XLA_FLAGS"] = "--xla_gpu_cuda_data_dir=/usr/lib/cuda"
torch.backends.cudnn.benchmark = True  # –£–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
torch.backends.cuda.matmul.allow_tf32 = True  # TF32 —É—Å–∫–æ—Ä—è–µ—Ç –º–∞—Ç–º—É–ª—å –Ω–∞ Ampere (T4)
torch.backends.cudnn.allow_tf32 = True
torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è fp16
torch.backends.cuda.preferred_linalg_library("cublas")  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è DDP
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_MODULE_LOADING"] = "LAZY"
os.environ["NVIDIA_TF32_OVERRIDE"] = "0"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
log_filename = f"training_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log"
logging.basicConfig(filename=log_filename, level=logging.INFO, 
                    format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

def main():
    torch.distributed.init_process_group(backend="nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    device = torch.device(f"cuda:{local_rank}")
    torch.cuda.set_device(device)
    logger.info(f"–ó–∞–ø—É—â–µ–Ω–æ –≤ —Ä–µ–∂–∏–º–µ DDP. LOCAL_RANK = {local_rank}")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer_path = "/kaggle/input/kaz-eng-rus/pytorch/default/1"  # –£–∫–∞–∂–∏ —Å–≤–æ–π –ø—É—Ç—å
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    model = AutoModelForMaskedLM.from_pretrained(
        "bert-base-multilingual-cased",
        ignore_mismatched_sizes=True
    )
    
    # üî• –í–∫–ª—é—á–∞–µ–º torch.compile –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    model = torch.compile(model, mode="max-autotune")
    model.to(device)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = load_dataset("json", data_files="/kaggle/input/kaz-rus-eng-wiki/train_pretrain.json")

    def tokenize_function(examples):
        inputs = tokenizer(
            examples["masked_sentence"], 
            truncation=True, max_length=128, padding="max_length"
        )
        inputs["labels"] = torch.tensor(inputs["input_ids"])  # Masked LM loss
        return inputs

    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    tokenized_dataset = dataset.map(
        tokenize_function, batched=True, 
        remove_columns=dataset["train"].column_names
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=32,  # üî• –£–≤–µ–ª–∏—á–µ–Ω batch size (T4 —Å–ø—Ä–∞–≤–∏—Ç—Å—è)
        per_device_eval_batch_size=32,
        gradient_accumulation_steps=4,  # üî• –£–º–µ–Ω—å—à–∞–µ—Ç VRAM usage
        learning_rate=5e-5,
        logging_steps=100,
        save_strategy="epoch",  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
        bf16=True,  # üî• bf16 –ª—É—á—à–µ –Ω–∞ T4, —á–µ–º fp16
        gradient_checkpointing=True,  # üî• –°–Ω–∏–∂–∞–µ—Ç VRAM –∑–∞ —Å—á–µ—Ç –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        dataloader_num_workers=8,  # –ë–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏
        report_to="none",
        evaluation_strategy="no",  # –û—Ç–∫–ª—é—á–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
        ddp_find_unused_parameters=False,  # üî• –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è DDP
    )

    # –°–æ–∑–¥–∞–µ–º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        tokenizer=tokenizer,  # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Ö–æ—Ç—è —É—Å—Ç–∞—Ä–µ–≤–∞–µ—Ç
    )

    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    train_result = trainer.train()
    logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    # –ó–∞–≤–µ—Ä—à–∞–µ–º DDP
    torch.distributed.destroy_process_group()

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
    plot_benchmarks(trainer.state.log_history)

def plot_benchmarks(log_history):
    sns.set(style="whitegrid")
    
    losses = [log["loss"] for log in log_history if "loss" in log]
    steps = list(range(len(losses)))

    plt.figure(figsize=(10, 6))
    plt.plot(steps, losses, label="Training Loss", color="blue")
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.title("Training Loss over Time")
    plt.legend()
    plt.savefig("training_loss.png")

    logger.info("–ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ training_loss.png")

if __name__ == "__main__":
    main()
